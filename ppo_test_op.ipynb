{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import model  # Make sure to import your custom environment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_state(state):\n",
    "    # Assuming `state` is a 2D array of shape (100000, 5)\n",
    "    compressed_state = np.mean(state, axis=0)\n",
    "    return compressed_state\n",
    "\n",
    "# Define the Actor-Critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 5  # Should be the processed state dimension, not the raw particle count\n",
    "action_dim = 74  # Number of actions\n",
    "hidden_dim = 256  # Number of hidden units\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99\n",
    "K_epochs = 10\n",
    "eps_clip = 0.2\n",
    "max_episodes = 500\n",
    "max_timesteps = 300\n",
    "episode_rewards = []\n",
    "episode_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, environment, waypoints, num_episodes=100):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations during inference\n",
    "    with torch.no_grad():\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            total_reward = 0\n",
    "            state = environment.reset()\n",
    "            belief = environment.particles.copy()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                belief_c = compress_state(belief)\n",
    "                action_probs, _ = model(belief_c)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "\n",
    "                # Take action in environment\n",
    "                next_state, next_belief, reward, done = environment.step(action.cpu().numpy(), state, waypoints)\n",
    "                total_reward += reward\n",
    "\n",
    "                belief = next_belief\n",
    "                state = next_state\n",
    "\n",
    "            print(f\"Episode {i_episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken is 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'waypoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maction taken is \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# env.render()  # Remove this line if your environment doesn't support rendering\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m next_state, next_belief, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), state, waypoints)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gokul/Desktop/multi_agent_research/multi-agent/ppo_test_op.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstate is \u001b[39m\u001b[39m{\u001b[39;00mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'waypoints' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "# import your environment model here\n",
    "# from your_environment_library import model\n",
    "import model\n",
    "\n",
    "env = model.UUV()\n",
    "env.initialize_particles()\n",
    "\n",
    "# Load the policy\n",
    "state_dim = 5\n",
    "action_dim = 74\n",
    "hidden_dim = 256\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "policy.load_state_dict(torch.load('model_weights/policy_ep_190.pt', map_location=device))\n",
    "\n",
    "# Evaluate the policy\n",
    "num_test_episodes = 1  # for instance, evaluate for 100 episodes\n",
    "test_rewards = []\n",
    "\n",
    "for i_episode in range(1, num_test_episodes+1):\n",
    "    state = env.reset()\n",
    "    belief = env.particles.copy()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        belief_c = compress_state(belief)\n",
    "        with torch.no_grad():  # Don't compute gradient for evaluations\n",
    "            action_probs, _ = policy(belief_c)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        print(f\"action taken is {action.item()}\")\n",
    "\n",
    "        # env.render()  # Remove this line if your environment doesn't support rendering\n",
    "\n",
    "        next_state, next_belief, reward, done = env.step(action.cpu().numpy(), state, waypoints)\n",
    "        episode_reward += reward\n",
    "\n",
    "        print(f\"state is {state}\")\n",
    "        print(f\"n state is {next_state}\")\n",
    "        print(f\"belief is {belief}\")\n",
    "        print(f\"n belief is {next_belief}\")\n",
    "\n",
    "        belief = next_belief\n",
    "        state = next_state\n",
    "\n",
    "    test_rewards.append(episode_reward)\n",
    "    print(f'Episode {i_episode} reward: {episode_reward}')\n",
    "\n",
    "avg_reward = np.mean(test_rewards)\n",
    "print(f'Average reward over {num_test_episodes} episodes: {avg_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
